{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop 3 - Scraping and Cleaning SOLUTIONS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X7zETQDMiUhr",
        "HTnlQlAFia5O",
        "vn6sMsglibLJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Jukn4qa4EO",
        "colab_type": "text"
      },
      "source": [
        "# MAIS Fall 2019 Workshop 3 - Scraping and Cleaning\n",
        "\n",
        "In this notebook, we will be walking through the basics of webscraping using the `requests`, `BeautifulSoup4`, and `selenium` python packages. With these packages at your disposal, you should be able to scrape both statically and dynamically loaded sites by the end of this workshop\n",
        "\n",
        "The content in this notebook is broken up into the following sections:\n",
        "\n",
        "1.   Introduction to `requests`, `BeautifulSoup4`, and DOM traversal\n",
        "2.   Scrape a simple statically loaded site with `requests` and `bs4`\n",
        "3.   Scrape a dynamically loaded site with `selenium` and `bs4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7zETQDMiUhr",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to requests, BeautifulSoup4, and DOM traversal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaJCKb0oj_8W",
        "colab_type": "text"
      },
      "source": [
        "We'll start by importing the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSeL_2LfahYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGtboHVdkGWm",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll play around a bit with these packages to get familiar with them. Specifically, we'll try scraping the 7-day weather forecast for Boston, MA.\n",
        "\n",
        "URL: https://forecast.weather.gov/MapClick.php?lat=42.3587&lon=-71.0567\n",
        "\n",
        "Start by checking out the website (use Inspect Element), and get a feel for the site's HTML structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfDufTKilB6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = 'https://forecast.weather.gov/MapClick.php?lat=42.3587&lon=-71.0567'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8ft4gGgk_PB",
        "colab_type": "text"
      },
      "source": [
        "Once you've checked out the site and know what you're looking for, it's time to get scraping! We'll start by using the `requests` package which, as we've already mentioned, lets us retrieve a site's HTML via a GET request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y515x9ZlYmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page = requests.get(URL)\n",
        "print(page)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpoVeuYllmu4",
        "colab_type": "text"
      },
      "source": [
        "A [200] response from a GET request is good news! The `Response` object's `content` field contains a byte string of the HTML of the website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JkDm4RAlsWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out the page's \"content\" field\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "print(page.content)\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibsmaN6qv9Hf",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the HTML is messy. Lucky for us we can use `BeautifulSoup4` to parse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXDw5kYdwEC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "print(soup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QckKt57BwP5v",
        "colab_type": "text"
      },
      "source": [
        "The `BeautifulSoup` object also comes with a handful of functions to traverse the html DOM tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJSUmM02xCcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'soup' is a BeautifulSoup instance\n",
        "print(type(soup))\n",
        "\n",
        "# it comes with a .children attribute which stores a list iterator\n",
        "print(type(soup.children))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1WlfD3Ex244",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's iterate through the immediate children of soup\n",
        "# print out the variable type of each child\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "for child in soup.children:\n",
        "  print(type(child))\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7NN9GQmwP9y",
        "colab_type": "text"
      },
      "source": [
        "As you can see, all the children of the HTML tree have been converted to `bs4` elements.\n",
        "\n",
        "Next, let's convert the iterator to a list so it's easier to work with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddeSG9IkwS24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "children = list(soup.children)\n",
        "print(children)\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC60_H9BT72A",
        "colab_type": "text"
      },
      "source": [
        "The third element of the children list contained the actual HTML document.\n",
        "\n",
        "Retrieve the third element (index of 2) and convert its elements to a list as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt_tKmla44zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "html = list(soup.children)[2]\n",
        "print(html)\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqeQMbp7449R",
        "colab_type": "text"
      },
      "source": [
        "We can keep traversing down this tree, until we reach an endpoint. Let's try parsing another child element one more time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPtRDKYh45Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a list of the children of the html bs4 element\n",
        "html_children = list(html.children)\n",
        "print(html_children)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpwOywbMU4zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the fourth element is the body of the HTML\n",
        "body = html_children[3]\n",
        "print(body)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsia_mCW45MY",
        "colab_type": "text"
      },
      "source": [
        "While we can keep traversing the tree to find what we want, this clearly isn't very efficient. The last thing we will cover in this section is how to access HTML elements by a known tag, id, or class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8ND9s3w45T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BeautifulSoup4 lets us find all instances of a tag\n",
        "# we can also find the first instance with just soup.find('p')\n",
        "p_tags = list(soup.find_all('p'))\n",
        "print(p_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbdsWpxNWXN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can search for all instances of an id\n",
        "forecast_list_id = list(soup.find\n",
        "                        _all(id='seven-day-forecast-list'))\n",
        "print(forecast_list_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7FyirNVhUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can search for all instances of a class too\n",
        "# and we can search for it in a previously\n",
        "tombstone_classes = list(soup.find_all(class_='forecast-tombstone'))\n",
        "print(tombstone_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDy5iugvg_gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lastly, we can use CSS combinator selectors with the 'select()' method\n",
        "# to learn more about CSS combinator selectors, visit\n",
        "# https://developer.mozilla.org/en-US/docs/Learn/CSS/Introduction_to_CSS/Combinators_and_multiple_selectors\n",
        "\n",
        "# this line finds all <p> elements in a <div> element\n",
        "p_in_div = list(soup.select(\"div p\"))\n",
        "print(p_in_div)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBUjRXckc9R",
        "colab_type": "text"
      },
      "source": [
        "Once we get to a leaf node in the HTML tree structure, we can use the `get_text()` function to retrieve the content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udf9FlEtkmak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(p_in_div[3].get_text())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTnlQlAFia5O",
        "colab_type": "text"
      },
      "source": [
        "## 2. Scrape a simple statically loaded site with requests and bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_095-9mXpyk",
        "colab_type": "text"
      },
      "source": [
        "We should now have enough knowledge to scrape the URL from the National Weather Service. From the previous section, we should also have a good idea of what elements in the HTML we should focus on. However, if you are unsure, you can always use the `Inspect Element` tool in Chrome to find the relevant tags/ids/classes.\n",
        "\n",
        "To begin, let's rerun our GET request and re-instantiate a new `soup` variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hho0GiEcXw98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = 'https://forecast.weather.gov/MapClick.php?lat=42.3587&lon=-71.0567'\n",
        "# send GET request\n",
        "page = requests.get(URL)\n",
        "# instantiate BeautifulSoup object\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkOmIJySYUSG",
        "colab_type": "text"
      },
      "source": [
        "For this scrape, we want to get the information stored in this container:\n",
        "\n",
        "![7-day-forecast](https://drive.google.com/uc?id=1sFbnCXFWaGZDDENo1bFRwTVW3ojZlDqS)\n",
        "\n",
        "Formatted in the following manner:\n",
        "\n",
        "period | short_desc | temp\n",
        "--- | --- | ---\n",
        "Tonight | Showers \\n Likely and \\n Patchy Fog | Low: 62 °F \n",
        "Monday | Mostly Sunny | High: 75 °F "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt9v3huWYTf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start by extracting the 7-day forecast panel shown in the first image above\n",
        "# Hint: find the id for this object, and use one of the functions covered in the previous section\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "seven_day = soup.find(id='seven-day-forecast')\n",
        "\n",
        "#----------------------\n",
        "\n",
        "# Within this new object, select all the \".period-name\" classes that are within \n",
        "# a \".tombstone-containers\" class and save it to a variable\n",
        "# Hint: CSS combinator selectors may be helpful here\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "period_tags = seven_day.select('.tombstone-container .period-name')\n",
        "\n",
        "#----------------------\n",
        "\n",
        "# In a similar manner to above, select all the \".short-desc\" classes that are in\n",
        "# a \".tombstone-containers\" class and save it to a variable\n",
        "# Hint: CSS combinator selectors may be helpful here\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "short_descs_tags = seven_day.select('.tombstone-container .short-desc')\n",
        "\n",
        "#----------------------\n",
        "\n",
        "# Once again, select all the \".temp\" classes that are in\n",
        "# a \".tombstone-containers\" class and save it to a variable\n",
        "# Hint: CSS combinator selectors may be helpful here\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "temps_tags = seven_day.select('.tombstone-container .temp')\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6iqZdLMnbSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print some of your results out to make sure you did it right\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "print(seven_day)\n",
        "print(period_tags)\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JqQBPjrk3Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now that we have all of our data in an iterable form (as bs4.ResultSet),\n",
        "# we can loop through the elements we want and use the get_text() method to retrieve\n",
        "# our data without HTML tags\n",
        "\n",
        "# loop through your \".period-name\" classes from above to get your period names\n",
        "# save the results to a list\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "periods = [pt.get_text() for pt in period_tags]\n",
        "\n",
        "#----------------------\n",
        "\n",
        "# loop through the \".short-desc\" classes from above to get your short descriptions\n",
        "# save the results to a list\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "short_descs = [sd.get_text() for sd in short_descs_tags]\n",
        "\n",
        "#----------------------\n",
        "\n",
        "# loop through the \".temp\" classes from above to get your temperatures\n",
        "# save the results to a list\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "temps = [t.get_text() for t in temps_tags]\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vtFNSEMialR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lastly, we combine these lists into a pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "weather = pd.DataFrame({\n",
        "    'period': periods, #--- YOUR CODE HERE ---,\n",
        "    'short_desc': short_descs, #--- YOUR CODE HERE ---,\n",
        "    'temp': temps, #--- YOUR CODE HERE ---\n",
        "})\n",
        "\n",
        "weather"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVTf88ekqgIb",
        "colab_type": "text"
      },
      "source": [
        "Congrats, you just scraped your first website!\n",
        "\n",
        "We can continue to process the data we just collected in many ways. For instance, we could extract the numerical value of the temperature from the `temps` column rather than leaving it as a string. However, we're not going to move onto scraping more advanced sites."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn6sMsglibLJ",
        "colab_type": "text"
      },
      "source": [
        "## 3. Scrape a dynamically loaded site with selenium and bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy2iufK0YoSn",
        "colab_type": "text"
      },
      "source": [
        "We now know that we can use `selenium` to automate web browsing. `selenium` works by instantiating a \"WebDriver\" (in this case, we will use the ChromeDriver) to run and interface with a web browser. Your code can then control what this browser does, such as clicking all of the \"Read More\" buttons on a page or sending keystrokes.\n",
        "\n",
        "**Selenium Documentation**:  \n",
        "https://selenium-python.readthedocs.io/  \n",
        "https://selenium-python.readthedocs.io/locating-elements.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1G_nXNZPmmF",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Installing Selenium\n",
        "\n",
        "To use `selenium`, we need to install the package and the Chrome WebDriver executable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ibLsDED_FPj",
        "colab_type": "code",
        "outputId": "6a5a13bd-cef9-4205-dcd8-93ca97ef3ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [73.5 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [782 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [700 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,718 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,304 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [827 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [995 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,234 B]\n",
            "Fetched 6,676 kB in 4s (1,827 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 70.8 MB of archives.\n",
            "After this operation, 254 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 77.0.3865.90-0ubuntu0.18.04.1 [1,079 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 77.0.3865.90-0ubuntu0.18.04.1 [62.3 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 77.0.3865.90-0ubuntu0.18.04.1 [3,011 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 77.0.3865.90-0ubuntu0.18.04.1 [4,388 kB]\n",
            "Fetched 70.8 MB in 5s (13.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 132681 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_77.0.3865.90-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Setting up chromium-browser (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuarrojwUoaM",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Simple HTML Page Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35UGzxdOpn2F",
        "colab_type": "text"
      },
      "source": [
        "Let's test out `selenium` on a simple website: https://duckduckgo.com/. First, start our Chrome web browser..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwuF9-SbV-Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "\n",
        "# Specify the configuration for the Chrome webdriver.\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--ignore-certificate-errors')\n",
        "options.add_argument('--incognito')\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "\n",
        "# Start the Chrome webdriver (i.e., the web browser).\n",
        "driver = webdriver\\\n",
        "    .Chrome(\n",
        "            executable_path = 'chromedriver',\n",
        "            options=options)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouHOqTZ7pjeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instruct the web driver to navigate to DuckDuckGo.\n",
        "URL = 'https://duckduckgo.com/'\n",
        "driver.get(URL)\n",
        "time.sleep(1)\n",
        "\n",
        "# Use selenium's API to find the HTML element representing the search bar,\n",
        "# and then type in our query.\n",
        "search_bar = driver.find_element_by_id('search_form_input_homepage')\n",
        "search_bar.send_keys('Hello World!')\n",
        "time.sleep(2)\n",
        "\n",
        "# Lastly, find the HTML element representing the 'Search' button and click it.\n",
        "search_button = driver.find_element_by_id('search_button_homepage')\n",
        "search_button.click()\n",
        "time.sleep(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1h_D8QWzgKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Once we are done interacting with the page, we can also download and save the \n",
        "# contents of the webpage we are currently on. We can also use BS4 to process\n",
        "# the HTML file afterwards if we desire.\n",
        "page_source = driver.page_source\n",
        "with open('duckduckgo.html','w', encoding='utf-8') as file:\n",
        "  file.write(page_source)\n",
        "driver.close()\n",
        "\n",
        "from google.colab import files\n",
        "files.download('duckduckgo.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCkhHjH9Uu-c",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 TripAdvisor Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhyIJ_Qj-UvV",
        "colab_type": "text"
      },
      "source": [
        "Next, lets repeat this exercise on a more complicated website..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwo69qUR9fhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--ignore-certificate-errors')\n",
        "options.add_argument('--incognito')\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "driver = webdriver\\\n",
        "    .Chrome(\n",
        "            executable_path = 'chromedriver',\n",
        "            options=options)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaPeRY0pBxix",
        "colab_type": "text"
      },
      "source": [
        "We can now use the webdriver's built-in functions to find and press all the \"Read More\" buttons.\n",
        "\n",
        "If we use \"Inspect Element\" on the TripAdvisor page, we'll see that the \"Read More\" buttons are all defined by the class with the name: <br>\n",
        "\"location-review-review-list-parts-ExpandableReview__cta--2mR2g\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsqPzGbI_s-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = 'https://www.tripadvisor.com/Airline_Review-d8729157-Reviews-Spirit-Airlines#REVIEWS'\n",
        "# get webpage\n",
        "driver.get(URL)\n",
        "\n",
        "# get a list of the \"Read More\" buttons; clicking the any of the buttons will expand all of the reviews\n",
        "time.sleep(2)\n",
        "read_more_buttons = driver.find_elements_by_xpath(\"//span[text()='Read more']\")\n",
        "for button in read_more_buttons:\n",
        "    if button.is_displayed():\n",
        "        button.click()\n",
        "        break\n",
        "      \n",
        "# save source HTML of page now that the JS has run\n",
        "page_source = driver.page_source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yJXktRHDEec",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the source HTML, the process is the same as before. We can use `bs4` to traverse the HTML DOM and get all the info we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf4gvhZwDMda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Use the BeautifulSoup \"find_all\" function to get the class that contains all the expanded reviews\n",
        "# Hint: you may need to open the page in your browser and find the correct class after pressing the \"Read More\" function\n",
        "\n",
        "#--- YOUR CODE HERE ---\n",
        "\n",
        "reviews_selector = soup.find_all('q', class_='location-review-review-list-parts-ExpandableReview__reviewText--gOmRC')\n",
        "\n",
        "#----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swQAE5MaPPXh",
        "colab_type": "text"
      },
      "source": [
        "When we begin scraping larger webpages, it's more memory efficient to write to a csv line-by-line, rather than constructing an entire dataframe and converting the entire variable to a csv. The example above is not too large to require this, but we'll try it for learning purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_28S4qqNzuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open the csv file to write to.\n",
        "with open('trip_advisor_scrape.txt','w') as file:\n",
        "  \n",
        "    # Loop through the bs4.ResultSet obtained from the previous cell\n",
        "    # for each entry in the list, get its text, use the strip() function\n",
        "    # to get rid of trailing whitespace, and then write it to the file with:\n",
        "          # file.write(line)\n",
        "          # file.write('\\n')\n",
        "    \n",
        "    #--- YOUR CODE HERE ---\n",
        "    for review_selector in reviews_selector:\n",
        "        line = review_selector.get_text().strip()\n",
        "        file.write(line)\n",
        "        file.write('\\n')\n",
        "    #----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZM0SRS9Rs3C",
        "colab_type": "text"
      },
      "source": [
        "Finally, since we're working in Google Collab, we need to download the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC9XLPGPS_X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('trip_advisor_scrape.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NttTSEEJOLAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}